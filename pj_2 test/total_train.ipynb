{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the data set\n",
    "from word_mapping import word_mapping\n",
    "from tag_mapping import tag_mapping\n",
    "from utils import *\n",
    "from check import check\n",
    "\n",
    "cn_train_data, cn_valid_data = get_data_set('Chinese')\n",
    "cn_TagMapping = tag_mapping('Chinese')\n",
    "cn_WordMapping = word_mapping(cn_train_data)\n",
    "cn_train_data.get_tag_mapping(cn_TagMapping.encode_mapping)\n",
    "cn_train_data.get_word_mapping(cn_WordMapping.encode_mapping)\n",
    "cn_valid_data.get_tag_mapping(cn_TagMapping.encode_mapping)\n",
    "cn_valid_data.get_word_mapping(cn_WordMapping.encode_mapping)\n",
    "\n",
    "en_train_data, en_valid_data = get_data_set('English')\n",
    "en_TagMapping = tag_mapping('English')\n",
    "en_WordMapping = word_mapping(en_train_data)\n",
    "en_train_data.get_tag_mapping(en_TagMapping.encode_mapping)\n",
    "en_train_data.get_word_mapping(en_WordMapping.encode_mapping)\n",
    "en_valid_data.get_tag_mapping(en_TagMapping.encode_mapping)\n",
    "en_valid_data.get_word_mapping(en_WordMapping.encode_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # the training of HMM\\n\\nfrom HMM import *\\n\\n# Chinese version\\ncn_HMM_path = 'HMM_model/HMM_Chinese.npz'\\ncn_model = HMM(cn_TagMapping.num_tag, cn_WordMapping.num_word)\\nsmoothing_factor = 1\\ncn_model.param_estimate(cn_train_data, smoothing_factor=smoothing_factor)\\ncn_model.save(cn_HMM_path)\\n\\n# English version\\nen_HMM_path = 'HMM_model/HMM_English.npz'\\nen_model = HMM(en_TagMapping.num_tag, en_WordMapping.num_word)\\nsmoothing_factor = 1\\nen_model.param_estimate(en_train_data, smoothing_factor=smoothing_factor)\\nen_model.save(en_HMM_path) \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # the training of HMM\n",
    "\n",
    "from HMM import *\n",
    "\n",
    "# Chinese version\n",
    "cn_HMM_path = 'HMM_model/HMM_Chinese.npz'\n",
    "cn_model = HMM(cn_TagMapping.num_tag, cn_WordMapping.num_word)\n",
    "smoothing_factor = 1\n",
    "cn_model.param_estimate(cn_train_data, smoothing_factor=smoothing_factor)\n",
    "cn_model.save(cn_HMM_path)\n",
    "\n",
    "# English version\n",
    "en_HMM_path = 'HMM_model/HMM_English.npz'\n",
    "en_model = HMM(en_TagMapping.num_tag, en_WordMapping.num_word)\n",
    "smoothing_factor = 1\n",
    "en_model.param_estimate(en_train_data, smoothing_factor=smoothing_factor)\n",
    "en_model.save(en_HMM_path) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" # Chinese version\\ncn_features = feature_functions(cn_train_data)\\ncn_model = CRF(cn_features, cn_TagMapping.num_tag)\\n\\ncn_model.train(cn_train_data, max_epoch=15)\\n\\nwith open('handmade_CRF_model/CRF_Chinese.pkl', 'wb') as f:\\n    pickle.dump(cn_model, f)\\n\\n#English version\\nen_features = feature_functions(en_train_data)\\nen_model = CRF(en_features, en_TagMapping.num_tag)\\n\\nen_model.train(en_train_data, max_epoch=15)\\n\\nwith open('handmade_CRF_model/CRF_English.pkl', 'wb') as f:\\n    pickle.dump(en_model, f)\\n \""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the training of my handmade CRF\n",
    "from my_handmade_CRF import feature_functions, CRF\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "\"\"\" # Chinese version\n",
    "cn_features = feature_functions(cn_train_data)\n",
    "cn_model = CRF(cn_features, cn_TagMapping.num_tag)\n",
    "\n",
    "cn_model.train(cn_train_data, max_epoch=15)\n",
    "\n",
    "with open('handmade_CRF_model/CRF_Chinese.pkl', 'wb') as f:\n",
    "    pickle.dump(cn_model, f)\n",
    "\n",
    "#English version\n",
    "en_features = feature_functions(en_train_data)\n",
    "en_model = CRF(en_features, en_TagMapping.num_tag)\n",
    "\n",
    "en_model.train(en_train_data, max_epoch=15)\n",
    "\n",
    "with open('handmade_CRF_model/CRF_English.pkl', 'wb') as f:\n",
    "    pickle.dump(en_model, f)\n",
    " \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the training of BiLSTM CRF\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import BiLSTM_CRF\n",
    "\n",
    "dropout = 0.5\n",
    "embed_size = 256\n",
    "hidden_size = 256\n",
    "batch_size = 32\n",
    "max_epoch = 20\n",
    "lr = 0.001\n",
    "clip_max_norm = 5.0\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 25.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Iter: 439, Loss: 10.4419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Iter: 439, Loss: 5.3788\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2, Iter: 439, Loss: 4.4156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3, Iter: 439, Loss: 2.5789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 26.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4, Iter: 439, Loss: 2.7480\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 25.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 5, Iter: 439, Loss: 2.0014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 26.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 6, Iter: 439, Loss: 2.8076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 26.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 7, Iter: 439, Loss: 2.1272\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 8, Iter: 439, Loss: 2.9104\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 9, Iter: 439, Loss: 3.2360\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10, Iter: 439, Loss: 2.2766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 11, Iter: 439, Loss: 2.5251\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 12, Iter: 439, Loss: 1.6173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 24.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 13, Iter: 439, Loss: 1.5987\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14, Iter: 439, Loss: 3.0432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 15, Iter: 439, Loss: 2.4836\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:17, 25.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 16, Iter: 439, Loss: 1.9829\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 25.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 17, Iter: 439, Loss: 2.4293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 26.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 18, Iter: 439, Loss: 1.8231\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "439it [00:16, 26.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 19, Iter: 439, Loss: 3.1246\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# English version\n",
    "en_tag_size = en_TagMapping.num_tag\n",
    "en_vocab_size = en_WordMapping.num_word\n",
    "\n",
    "en_model = BiLSTM_CRF.BiLSTMCRF(en_tag_size, en_vocab_size, dropout, embed_size, hidden_size).to(device)\n",
    "for name, param in en_model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        nn.init.normal_(param.data, 0, 0.01)\n",
    "    else:\n",
    "        nn.init.constant_(param.data, 0)\n",
    "optimizer = torch.optim.Adam(en_model.parameters(), lr=lr)\n",
    "for epoch in range(max_epoch):\n",
    "    num_iter = 0\n",
    "    for sentences, tags in tqdm(batch_iter(en_train_data, batch_size=batch_size)):\n",
    "        num_iter += 1\n",
    "        sentences, sent_lengths = pad(sentences, en_vocab_size - 1, device)\n",
    "        tags, _ = pad(tags, en_tag_size - 1, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = en_model(sentences, tags, sent_lengths)\n",
    "        loss = batch_loss.mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(en_model.parameters(), max_norm=clip_max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch: %d, Iter: %d, Loss: %.4f' % (epoch, num_iter, loss.item()))\n",
    "\n",
    "en_model_save_path = 'BiLSTM_CRF/English.pth'\n",
    "en_model.save(en_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" cn_tag_size = cn_TagMapping.num_tag\\ncn_vocab_size = cn_WordMapping.num_word\\n\\ncn_model = BiLSTM_CRF.BiLSTMCRF(cn_tag_size, cn_vocab_size, dropout, embed_size, hidden_size).to(device)\\nfor name, param in cn_model.named_parameters():\\n    if 'weight' in name:\\n        nn.init.normal_(param.data, 0, 0.01)\\n    else:\\n        nn.init.constant_(param.data, 0)\\noptimizer = torch.optim.Adam(cn_model.parameters(), lr=lr)\\nfor epoch in range(max_epoch):\\n    num_iter = 0\\n    for sentences, tags in tqdm(batch_iter(cn_train_data, batch_size=batch_size)):\\n        num_iter += 1\\n        sentences, sent_lengths = pad(sentences, cn_vocab_size - 1, device)\\n        tags, _ = pad(tags, cn_tag_size - 1, device)\\n\\n        optimizer.zero_grad()\\n        batch_loss = cn_model(sentences, tags, sent_lengths)\\n        loss = batch_loss.mean()\\n        loss.backward()\\n        torch.nn.utils.clip_grad_norm_(cn_model.parameters(), max_norm=clip_max_norm)\\n        optimizer.step()\\n\\n    print('Epoch: %d, Iter: %d, Loss: %.4f' % (epoch, num_iter, loss.item()))\\n\\ncn_model_save_path = 'BiLSTM_CRF/Chinese.pth'\\ncn_model.save(cn_model_save_path) \""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" cn_tag_size = cn_TagMapping.num_tag\n",
    "cn_vocab_size = cn_WordMapping.num_word\n",
    "\n",
    "cn_model = BiLSTM_CRF.BiLSTMCRF(cn_tag_size, cn_vocab_size, dropout, embed_size, hidden_size).to(device)\n",
    "for name, param in cn_model.named_parameters():\n",
    "    if 'weight' in name:\n",
    "        nn.init.normal_(param.data, 0, 0.01)\n",
    "    else:\n",
    "        nn.init.constant_(param.data, 0)\n",
    "optimizer = torch.optim.Adam(cn_model.parameters(), lr=lr)\n",
    "for epoch in range(max_epoch):\n",
    "    num_iter = 0\n",
    "    for sentences, tags in tqdm(batch_iter(cn_train_data, batch_size=batch_size)):\n",
    "        num_iter += 1\n",
    "        sentences, sent_lengths = pad(sentences, cn_vocab_size - 1, device)\n",
    "        tags, _ = pad(tags, cn_tag_size - 1, device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        batch_loss = cn_model(sentences, tags, sent_lengths)\n",
    "        loss = batch_loss.mean()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(cn_model.parameters(), max_norm=clip_max_norm)\n",
    "        optimizer.step()\n",
    "\n",
    "    print('Epoch: %d, Iter: %d, Loss: %.4f' % (epoch, num_iter, loss.item()))\n",
    "\n",
    "cn_model_save_path = 'BiLSTM_CRF/Chinese.pth'\n",
    "cn_model.save(cn_model_save_path) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' # the training of transformer\\nfrom my_transformer import Transformer\\n\\ndim_embed = 100\\nmax_len = 300\\nnum_heads = 5\\ndim_feedforward = 400\\ndropout = 0\\nnum_encoder = 6\\n\\nlr = 0.001\\nmax_epoch = 30\\nbatch_size = 32\\nloss_fn = nn.CrossEntropyLoss()\\n\\n# Chinese version\\ncn_model = Transformer(\\n    dim_embed,\\n    cn_vocab_size,\\n    max_len,\\n    num_encoder,\\n    num_heads,\\n    dim_feedforward,\\n    cn_tag_size,\\n    dropout)\\ncn_model.to(device)\\noptimizer = torch.optim.Adam(cn_model.parameters(), lr=lr)\\nfor epoch in range(max_epoch):\\n    loss_sum = 0\\n    num = 0\\n    for sentences, tags in tqdm(batch_iter(cn_train_data, batch_size=batch_size)):\\n        sentences, sent_lengths = pad(sentences, cn_vocab_size - 1, device)\\n        tags, _ = pad(tags, cn_tag_size - 1, device)\\n\\n        mask_sentences = torch.ones_like(sentences)\\n        mask_sentences[sentences == cn_vocab_size - 1] = 0\\n        mask_tags = torch.ones_like(tags)\\n        mask_tags[tags == cn_tag_size - 1] = 0\\n\\n        optimizer.zero_grad()\\n        \\n        output = cn_model(sentences, mask_sentences)\\n        output = output.view(-1, output.shape[-1])\\n        tags = tags.view(-1)\\n        loss = loss_fn(output, tags)\\n        loss_sum += loss.item()\\n        num += 1\\n        loss.backward()\\n        optimizer.step()\\n    print(f\"epoch: {epoch}, loss: {loss_sum / num}\")\\n\\ncn_model_save_path = \"transformer/Chinese.pkl\"\\nwith open(cn_model_save_path, \"wb\") as f:\\n    pickle.dump(cn_model, f)\\n\\n\\n#English version\\nen_model = Transformer(\\n    dim_embed,\\n    en_vocab_size,\\n    max_len,\\n    num_encoder,\\n    num_heads,\\n    dim_feedforward,\\n    en_tag_size,\\n    dropout)\\nen_model.to(device)\\noptimizer = torch.optim.Adam(en_model.parameters(), lr=lr)\\nfor epoch in range(max_epoch):\\n    loss_sum = 0\\n    num = 0\\n    for sentences, tags in tqdm(batch_iter(en_train_data, batch_size=batch_size)):\\n        sentences, sent_lengths = pad(sentences, en_vocab_size - 1, device)\\n        tags, _ = pad(tags, en_tag_size - 1, device)\\n\\n        mask_sentences = torch.ones_like(sentences)\\n        mask_sentences[sentences == en_vocab_size - 1] = 0\\n        mask_tags = torch.ones_like(tags)\\n        mask_tags[tags == en_tag_size - 1] = 0\\n\\n        optimizer.zero_grad()\\n        \\n        output = en_model(sentences, mask_sentences)\\n        output = output.view(-1, output.shape[-1])\\n        tags = tags.view(-1)\\n        loss = loss_fn(output, tags)\\n        loss_sum += loss.item()\\n        num += 1\\n        loss.backward()\\n        optimizer.step()\\n    print(f\"epoch: {epoch}, loss: {loss_sum / num}\")\\n\\nen_model_save_path = \"transformer/English.pkl\"\\nwith open(en_model_save_path, \"wb\") as f:\\n    pickle.dump(en_model, f) '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" # the training of transformer\n",
    "from my_transformer import Transformer\n",
    "\n",
    "dim_embed = 100\n",
    "max_len = 300\n",
    "num_heads = 5\n",
    "dim_feedforward = 400\n",
    "dropout = 0\n",
    "num_encoder = 6\n",
    "\n",
    "lr = 0.001\n",
    "max_epoch = 30\n",
    "batch_size = 32\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "# Chinese version\n",
    "cn_model = Transformer(\n",
    "    dim_embed,\n",
    "    cn_vocab_size,\n",
    "    max_len,\n",
    "    num_encoder,\n",
    "    num_heads,\n",
    "    dim_feedforward,\n",
    "    cn_tag_size,\n",
    "    dropout)\n",
    "cn_model.to(device)\n",
    "optimizer = torch.optim.Adam(cn_model.parameters(), lr=lr)\n",
    "for epoch in range(max_epoch):\n",
    "    loss_sum = 0\n",
    "    num = 0\n",
    "    for sentences, tags in tqdm(batch_iter(cn_train_data, batch_size=batch_size)):\n",
    "        sentences, sent_lengths = pad(sentences, cn_vocab_size - 1, device)\n",
    "        tags, _ = pad(tags, cn_tag_size - 1, device)\n",
    "\n",
    "        mask_sentences = torch.ones_like(sentences)\n",
    "        mask_sentences[sentences == cn_vocab_size - 1] = 0\n",
    "        mask_tags = torch.ones_like(tags)\n",
    "        mask_tags[tags == cn_tag_size - 1] = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = cn_model(sentences, mask_sentences)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = loss_fn(output, tags)\n",
    "        loss_sum += loss.item()\n",
    "        num += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch: {epoch}, loss: {loss_sum / num}\")\n",
    "\n",
    "cn_model_save_path = \"transformer/Chinese.pkl\"\n",
    "with open(cn_model_save_path, \"wb\") as f:\n",
    "    pickle.dump(cn_model, f)\n",
    "\n",
    "\n",
    "#English version\n",
    "en_model = Transformer(\n",
    "    dim_embed,\n",
    "    en_vocab_size,\n",
    "    max_len,\n",
    "    num_encoder,\n",
    "    num_heads,\n",
    "    dim_feedforward,\n",
    "    en_tag_size,\n",
    "    dropout)\n",
    "en_model.to(device)\n",
    "optimizer = torch.optim.Adam(en_model.parameters(), lr=lr)\n",
    "for epoch in range(max_epoch):\n",
    "    loss_sum = 0\n",
    "    num = 0\n",
    "    for sentences, tags in tqdm(batch_iter(en_train_data, batch_size=batch_size)):\n",
    "        sentences, sent_lengths = pad(sentences, en_vocab_size - 1, device)\n",
    "        tags, _ = pad(tags, en_tag_size - 1, device)\n",
    "\n",
    "        mask_sentences = torch.ones_like(sentences)\n",
    "        mask_sentences[sentences == en_vocab_size - 1] = 0\n",
    "        mask_tags = torch.ones_like(tags)\n",
    "        mask_tags[tags == en_tag_size - 1] = 0\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        output = en_model(sentences, mask_sentences)\n",
    "        output = output.view(-1, output.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "        loss = loss_fn(output, tags)\n",
    "        loss_sum += loss.item()\n",
    "        num += 1\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"epoch: {epoch}, loss: {loss_sum / num}\")\n",
    "\n",
    "en_model_save_path = \"transformer/English.pkl\"\n",
    "with open(en_model_save_path, \"wb\") as f:\n",
    "    pickle.dump(en_model, f) \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
